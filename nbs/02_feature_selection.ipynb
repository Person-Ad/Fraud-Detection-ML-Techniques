{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af231b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\",       # Background style (\"whitegrid\", \"darkgrid\", etc.)\n",
    "    palette=\"deep\",          # Default color palette (\"deep\", \"muted\", \"bright\", etc.)\n",
    "    font=\"sans-serif\",       # Font family\n",
    "    font_scale=1.1,          # Scale font size slightly\n",
    "    rc={\"figure.figsize\": (8, 5)}  # Default figure size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69df4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../datasets\")\n",
    "\n",
    "train_identity = pd.read_csv(dataset_path / \"train_identity.csv\")\n",
    "train_tx = pd.read_csv(dataset_path / \"train_transaction.csv\")\n",
    "\n",
    "test_identity = pd.read_csv(dataset_path / \"test_identity.csv\")\n",
    "test_tx = pd.read_csv(dataset_path / \"test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9a52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_cols = pd.merge(train_tx, train_identity, on='TransactionID', how='left')\n",
    "# test = pd.merge(train_tx, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "X =  train_all_cols.drop(columns=['isFraud', 'TransactionID'])\n",
    "y = train_all_cols['isFraud']\n",
    "X = X.fillna(-999) #* for lightgbm to handl\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3cc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b25a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cat_cols] = X_train[cat_cols].astype('category')\n",
    "X_val[cat_cols] = X_val[cat_cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9986e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_cols),\n",
    "        ('cat', 'passthrough', cat_cols)  # pass cat columns as-is\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([('preprocessor', preprocessor)])\n",
    "# X_train_ = pipeline.fit_transform(X_train)\n",
    "# X_val_ = pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e04d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for LGB\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "\n",
    "# Train\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_val],\n",
    "    num_boost_round=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2234f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "lgb.train(\n",
      "    params: Dict[str, Any],\n",
      "    train_set: lightgbm.basic.Dataset,\n",
      "    num_boost_round: int = \u001b[32m100\u001b[39m,\n",
      "    valid_sets: Optional[List[lightgbm.basic.Dataset]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    valid_names: Optional[List[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    feval: Union[Callable[[numpy.ndarray, lightgbm.basic.Dataset], Tuple[str, float, bool]], Callable[[numpy.ndarray, lightgbm.basic.Dataset], List[Tuple[str, float, bool]]], List[Union[Callable[[numpy.ndarray, lightgbm.basic.Dataset], Tuple[str, float, bool]], Callable[[numpy.ndarray, lightgbm.basic.Dataset], List[Tuple[str, float, bool]]]]], NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    init_model: Union[str, pathlib._local.Path, lightgbm.basic.Booster, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_training_booster: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    callbacks: Optional[List[Callable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> lightgbm.basic.Booster\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "\u001b[38;5;28;01mdef\u001b[39;00m train(\n",
      "    params: Dict[str, Any],\n",
      "    train_set: Dataset,\n",
      "    num_boost_round: int = \u001b[32m100\u001b[39m,\n",
      "    valid_sets: Optional[List[Dataset]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    valid_names: Optional[List[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    feval: Optional[Union[_LGBM_CustomMetricFunction, List[_LGBM_CustomMetricFunction]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    init_model: Optional[Union[str, Path, Booster]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    keep_training_booster: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    callbacks: Optional[List[Callable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> Booster:\n",
      "    \u001b[33m\"\"\"Perform the training with given parameters.\u001b[39m\n",
      "\n",
      "\u001b[33m    Parameters\u001b[39m\n",
      "\u001b[33m    ----------\u001b[39m\n",
      "\u001b[33m    params : dict\u001b[39m\n",
      "\u001b[33m        Parameters for training. Values passed through ``params`` take precedence over those\u001b[39m\n",
      "\u001b[33m        supplied via arguments.\u001b[39m\n",
      "\u001b[33m    train_set : Dataset\u001b[39m\n",
      "\u001b[33m        Data to be trained on.\u001b[39m\n",
      "\u001b[33m    num_boost_round : int, optional (default=100)\u001b[39m\n",
      "\u001b[33m        Number of boosting iterations.\u001b[39m\n",
      "\u001b[33m    valid_sets : list of Dataset, or None, optional (default=None)\u001b[39m\n",
      "\u001b[33m        List of data to be evaluated on during training.\u001b[39m\n",
      "\u001b[33m    valid_names : list of str, or None, optional (default=None)\u001b[39m\n",
      "\u001b[33m        Names of ``valid_sets``.\u001b[39m\n",
      "\u001b[33m    feval : callable, list of callable, or None, optional (default=None)\u001b[39m\n",
      "\u001b[33m        Customized evaluation function.\u001b[39m\n",
      "\u001b[33m        Each evaluation function should accept two parameters: preds, eval_data,\u001b[39m\n",
      "\u001b[33m        and return (eval_name, eval_result, is_higher_better) or list of such tuples.\u001b[39m\n",
      "\n",
      "\u001b[33m            preds : numpy 1-D array or numpy 2-D array (for multi-class task)\u001b[39m\n",
      "\u001b[33m                The predicted values.\u001b[39m\n",
      "\u001b[33m                For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes].\u001b[39m\n",
      "\u001b[33m                If custom objective function is used, predicted values are returned before any transformation,\u001b[39m\n",
      "\u001b[33m                e.g. they are raw margin instead of probability of positive class for binary task in this case.\u001b[39m\n",
      "\u001b[33m            eval_data : Dataset\u001b[39m\n",
      "\u001b[33m                A ``Dataset`` to evaluate.\u001b[39m\n",
      "\u001b[33m            eval_name : str\u001b[39m\n",
      "\u001b[33m                The name of evaluation function (without whitespaces).\u001b[39m\n",
      "\u001b[33m            eval_result : float\u001b[39m\n",
      "\u001b[33m                The eval result.\u001b[39m\n",
      "\u001b[33m            is_higher_better : bool\u001b[39m\n",
      "\u001b[33m                Is eval result higher better, e.g. AUC is ``is_higher_better``.\u001b[39m\n",
      "\n",
      "\u001b[33m        To ignore the default metric corresponding to the used objective,\u001b[39m\n",
      "\u001b[33m        set the ``metric`` parameter to the string ``\"None\"`` in ``params``.\u001b[39m\n",
      "\u001b[33m    init_model : str, pathlib.Path, Booster or None, optional (default=None)\u001b[39m\n",
      "\u001b[33m        Filename of LightGBM model or Booster instance used for continue training.\u001b[39m\n",
      "\u001b[33m    keep_training_booster : bool, optional (default=False)\u001b[39m\n",
      "\u001b[33m        Whether the returned Booster will be used to keep training.\u001b[39m\n",
      "\u001b[33m        If False, the returned value will be converted into _InnerPredictor before returning.\u001b[39m\n",
      "\u001b[33m        This means you won't be able to use ``eval``, ``eval_train`` or ``eval_valid`` methods of the returned Booster.\u001b[39m\n",
      "\u001b[33m        When your model is very large and cause the memory error,\u001b[39m\n",
      "\u001b[33m        you can try to set this param to ``True`` to avoid the model conversion performed during the internal call of ``model_to_string``.\u001b[39m\n",
      "\u001b[33m        You can still use _InnerPredictor as ``init_model`` for future continue training.\u001b[39m\n",
      "\u001b[33m    callbacks : list of callable, or None, optional (default=None)\u001b[39m\n",
      "\u001b[33m        List of callback functions that are applied at each iteration.\u001b[39m\n",
      "\u001b[33m        See Callbacks in Python API for more information.\u001b[39m\n",
      "\n",
      "\u001b[33m    Note\u001b[39m\n",
      "\u001b[33m    ----\u001b[39m\n",
      "\u001b[33m    A custom objective function can be provided for the ``objective`` parameter.\u001b[39m\n",
      "\u001b[33m    It should accept two parameters: preds, train_data and return (grad, hess).\u001b[39m\n",
      "\n",
      "\u001b[33m        preds : numpy 1-D array or numpy 2-D array (for multi-class task)\u001b[39m\n",
      "\u001b[33m            The predicted values.\u001b[39m\n",
      "\u001b[33m            Predicted values are returned before any transformation,\u001b[39m\n",
      "\u001b[33m            e.g. they are raw margin instead of probability of positive class for binary task.\u001b[39m\n",
      "\u001b[33m        train_data : Dataset\u001b[39m\n",
      "\u001b[33m            The training dataset.\u001b[39m\n",
      "\u001b[33m        grad : numpy 1-D array or numpy 2-D array (for multi-class task)\u001b[39m\n",
      "\u001b[33m            The value of the first order derivative (gradient) of the loss\u001b[39m\n",
      "\u001b[33m            with respect to the elements of preds for each sample point.\u001b[39m\n",
      "\u001b[33m        hess : numpy 1-D array or numpy 2-D array (for multi-class task)\u001b[39m\n",
      "\u001b[33m            The value of the second order derivative (Hessian) of the loss\u001b[39m\n",
      "\u001b[33m            with respect to the elements of preds for each sample point.\u001b[39m\n",
      "\n",
      "\u001b[33m    For multi-class task, preds are numpy 2-D array of shape = [n_samples, n_classes],\u001b[39m\n",
      "\u001b[33m    and grad and hess should be returned in the same format.\u001b[39m\n",
      "\n",
      "\u001b[33m    Returns\u001b[39m\n",
      "\u001b[33m    -------\u001b[39m\n",
      "\u001b[33m    booster : Booster\u001b[39m\n",
      "\u001b[33m        The trained Booster model.\u001b[39m\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(train_set, Dataset):\n",
      "        \u001b[38;5;28;01mraise\u001b[39;00m TypeError(f\"train() only accepts Dataset object, train_set has type '{type(train_set).__name__}'.\")\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m isinstance(valid_sets, list):\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m i, valid_item \u001b[38;5;28;01min\u001b[39;00m enumerate(valid_sets):\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(valid_item, Dataset):\n",
      "                \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
      "                    \u001b[33m\"Every item in valid_sets must be a Dataset object. \"\u001b[39m\n",
      "                    f\"Item {i} has type '{type(valid_item).__name__}'.\"\n",
      "                )\n",
      "\n",
      "    \u001b[38;5;66;03m# create predictor first\u001b[39;00m\n",
      "    params = copy.deepcopy(params)\n",
      "    params = _choose_param_value(\n",
      "        main_param_name=\u001b[33m\"objective\"\u001b[39m,\n",
      "        params=params,\n",
      "        default_value=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    )\n",
      "    fobj: Optional[_LGBM_CustomObjectiveFunction] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m callable(params[\u001b[33m\"objective\"\u001b[39m]):\n",
      "        fobj = params[\u001b[33m\"objective\"\u001b[39m]\n",
      "        params[\u001b[33m\"objective\"\u001b[39m] = \u001b[33m\"none\"\u001b[39m\n",
      "\n",
      "    params = _choose_num_iterations(num_boost_round_kwarg=num_boost_round, params=params)\n",
      "    num_boost_round = params[\u001b[33m\"num_iterations\"\u001b[39m]\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m num_boost_round <= \u001b[32m0\u001b[39m:\n",
      "        \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"Number of boosting rounds must be greater than 0. Got {num_boost_round}.\")\n",
      "\n",
      "    \u001b[38;5;66;03m# setting early stopping via global params should be possible\u001b[39;00m\n",
      "    params = _choose_param_value(\n",
      "        main_param_name=\u001b[33m\"early_stopping_round\"\u001b[39m,\n",
      "        params=params,\n",
      "        default_value=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    )\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[33m\"early_stopping_round\"\u001b[39m] \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        params.pop(\u001b[33m\"early_stopping_round\"\u001b[39m)\n",
      "    first_metric_only = params.get(\u001b[33m\"first_metric_only\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\n",
      "    predictor: Optional[_InnerPredictor] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m isinstance(init_model, (str, Path)):\n",
      "        predictor = _InnerPredictor.from_model_file(model_file=init_model, pred_parameter=params)\n",
      "    \u001b[38;5;28;01melif\u001b[39;00m isinstance(init_model, Booster):\n",
      "        predictor = _InnerPredictor.from_booster(booster=init_model, pred_parameter=dict(init_model.params, **params))\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m predictor \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        init_iteration = predictor.current_iteration()\n",
      "    \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "        init_iteration = \u001b[32m0\u001b[39m\n",
      "\n",
      "    train_set._update_params(params)._set_predictor(predictor)\n",
      "\n",
      "    is_valid_contain_train = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "    train_data_name = \u001b[33m\"training\"\u001b[39m\n",
      "    reduced_valid_sets = []\n",
      "    name_valid_sets = []\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(valid_sets, Dataset):\n",
      "            valid_sets = [valid_sets]\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m isinstance(valid_names, str):\n",
      "            valid_names = [valid_names]\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m i, valid_data \u001b[38;5;28;01min\u001b[39;00m enumerate(valid_sets):\n",
      "            \u001b[38;5;66;03m# reduce cost for prediction training data\u001b[39;00m\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m valid_data \u001b[38;5;28;01mis\u001b[39;00m train_set:\n",
      "                is_valid_contain_train = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m valid_names \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "                    train_data_name = valid_names[i]\n",
      "                \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "            reduced_valid_sets.append(valid_data._update_params(params).set_reference(train_set))\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m valid_names \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m len(valid_names) > i:\n",
      "                name_valid_sets.append(valid_names[i])\n",
      "            \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "                name_valid_sets.append(f\"valid_{i}\")\n",
      "    \u001b[38;5;66;03m# process callbacks\u001b[39;00m\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        callbacks_set = set()\n",
      "    \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m i, cb \u001b[38;5;28;01min\u001b[39;00m enumerate(callbacks):\n",
      "            cb.__dict__.setdefault(\u001b[33m\"order\"\u001b[39m, i - len(callbacks))\n",
      "        callbacks_set = set(callbacks)\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m callback._should_enable_early_stopping(params.get(\u001b[33m\"early_stopping_round\"\u001b[39m, \u001b[32m0\u001b[39m)):\n",
      "        callbacks_set.add(\n",
      "            callback.early_stopping(\n",
      "                stopping_rounds=params[\u001b[33m\"early_stopping_round\"\u001b[39m],  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "                first_metric_only=first_metric_only,\n",
      "                min_delta=params.get(\u001b[33m\"early_stopping_min_delta\"\u001b[39m, \u001b[32m0.0\u001b[39m),\n",
      "                verbose=_choose_param_value(\n",
      "                    main_param_name=\u001b[33m\"verbosity\"\u001b[39m,\n",
      "                    params=params,\n",
      "                    default_value=\u001b[32m1\u001b[39m,\n",
      "                ).pop(\u001b[33m\"verbosity\"\u001b[39m)\n",
      "                > \u001b[32m0\u001b[39m,\n",
      "            )\n",
      "        )\n",
      "\n",
      "    callbacks_before_iter_set = {cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;28;01min\u001b[39;00m callbacks_set \u001b[38;5;28;01mif\u001b[39;00m getattr(cb, \u001b[33m\"before_iteration\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)}\n",
      "    callbacks_after_iter_set = callbacks_set - callbacks_before_iter_set\n",
      "    callbacks_before_iter = sorted(callbacks_before_iter_set, key=attrgetter(\u001b[33m\"order\"\u001b[39m))\n",
      "    callbacks_after_iter = sorted(callbacks_after_iter_set, key=attrgetter(\u001b[33m\"order\"\u001b[39m))\n",
      "\n",
      "    \u001b[38;5;66;03m# construct booster\u001b[39;00m\n",
      "    \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "        booster = Booster(params=params, train_set=train_set)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n",
      "            booster.set_train_data_name(train_data_name)\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m valid_set, name_valid_set \u001b[38;5;28;01min\u001b[39;00m zip(reduced_valid_sets, name_valid_sets):\n",
      "            booster.add_valid(valid_set, name_valid_set)\n",
      "    \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "        train_set._reverse_update_params()\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m valid_set \u001b[38;5;28;01min\u001b[39;00m reduced_valid_sets:\n",
      "            valid_set._reverse_update_params()\n",
      "    booster.best_iteration = \u001b[32m0\u001b[39m\n",
      "\n",
      "    \u001b[38;5;66;03m# start training\u001b[39;00m\n",
      "    \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;28;01min\u001b[39;00m range(init_iteration, init_iteration + num_boost_round):\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;28;01min\u001b[39;00m callbacks_before_iter:\n",
      "            cb(\n",
      "                callback.CallbackEnv(\n",
      "                    model=booster,\n",
      "                    params=params,\n",
      "                    iteration=i,\n",
      "                    begin_iteration=init_iteration,\n",
      "                    end_iteration=init_iteration + num_boost_round,\n",
      "                    evaluation_result_list=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "                )\n",
      "            )\n",
      "\n",
      "        booster.update(fobj=fobj)\n",
      "\n",
      "        evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n",
      "        \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m valid_sets \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n",
      "                evaluation_result_list.extend(booster.eval_train(feval))\n",
      "            evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;28;01min\u001b[39;00m callbacks_after_iter:\n",
      "                cb(\n",
      "                    callback.CallbackEnv(\n",
      "                        model=booster,\n",
      "                        params=params,\n",
      "                        iteration=i,\n",
      "                        begin_iteration=init_iteration,\n",
      "                        end_iteration=init_iteration + num_boost_round,\n",
      "                        evaluation_result_list=evaluation_result_list,\n",
      "                    )\n",
      "                )\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m callback.EarlyStopException \u001b[38;5;28;01mas\u001b[39;00m earlyStopException:\n",
      "            booster.best_iteration = earlyStopException.best_iteration + \u001b[32m1\u001b[39m\n",
      "            evaluation_result_list = earlyStopException.best_score\n",
      "            \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "    booster.best_score = defaultdict(OrderedDict)\n",
      "    \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, eval_name, score, _ \u001b[38;5;28;01min\u001b[39;00m evaluation_result_list:\n",
      "        booster.best_score[dataset_name][eval_name] = score\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m keep_training_booster:\n",
      "        booster.model_from_string(booster.model_to_string()).free_dataset()\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m booster\n",
      "\u001b[31mFile:\u001b[39m      c:\\users\\ahmed osama\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\lightgbm\\engine.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "??lgb.train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
