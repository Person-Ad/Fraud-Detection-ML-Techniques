{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af231b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from evaluation import evaluate_model\n",
    "from preprocessing import *\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"whitegrid\",       # Background style (\"whitegrid\", \"darkgrid\", etc.)\n",
    "    palette=\"deep\",          # Default color palette (\"deep\", \"muted\", \"bright\", etc.)\n",
    "    font=\"sans-serif\",       # Font family\n",
    "    font_scale=1.1,          # Scale font size slightly\n",
    "    rc={\"figure.figsize\": (8, 5)}  # Default figure size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69df4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path(\"../datasets\")\n",
    "\n",
    "train_identity = pd.read_csv(dataset_path / \"train_identity.csv\")\n",
    "train_tx = pd.read_csv(dataset_path / \"train_transaction.csv\")\n",
    "\n",
    "# test_identity = pd.read_csv(dataset_path / \"test_identity.csv\")\n",
    "# test_tx = pd.read_csv(dataset_path / \"test_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9a52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_cols = pd.merge(train_tx, train_identity, on='TransactionID', how='left')\n",
    "# test_all_cols = pd.merge(train_tx, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "X =  train_all_cols.drop(columns=['isFraud'])\n",
    "y = train_all_cols['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc00eb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape: (590540, 433)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f5c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dbc916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test = run_feature_engineering(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c283ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_26', 'V305', 'V108', 'V113', 'V111', 'V118', 'V122', 'id_23', 'V123', 'V300', 'id_08', 'V115', 'V110', 'id_22', 'V107', 'V116', 'V311', 'V286', 'id_21', 'C3', 'V109', 'V117', 'id_27', 'V112', 'V120', 'id_07', 'V119', 'V121', 'id_25', 'id_24', 'V301', 'V114']\n",
      "(377945, 400)\n",
      "(118108, 400)\n"
     ]
    }
   ],
   "source": [
    "high_missing_cols = [col for col in X_train.columns if X_train[col].isnull().sum() / X_train.shape[0] > 0.96]\n",
    "high_missing_cols_X_test = [col for col in X_test.columns if X_test[col].isnull().sum() / X_test.shape[0] > 0.96]\n",
    "\n",
    "big_top_value_cols = [col for col in X_train.columns if X_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.96]\n",
    "big_top_value_cols_X_test = [col for col in X_test.columns if X_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.96]\n",
    "\n",
    "cols_to_drop = list(set(high_missing_cols + high_missing_cols_X_test + big_top_value_cols + big_top_value_cols_X_test ))\n",
    "len(cols_to_drop)\n",
    "print(cols_to_drop)\n",
    "\n",
    "\n",
    "X_train = X_train.drop(cols_to_drop, axis=1)\n",
    "X_test = X_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "\n",
    "X_train.drop('TransactionDT', axis=1, inplace=True)\n",
    "X_test.drop('TransactionDT', axis=1, inplace=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Label Encoding\n",
    "for f in X_train.columns:\n",
    "    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "        X_test[f] = lbl.transform(list(X_test[f].values)) \n",
    "        \n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa76d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "# print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "# print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
    "\n",
    "# sm = KMeansSMOTE(random_state=99, sampling_strategy = 0.15,  k_neighbors = 10,cluster_balance_threshold = 0.02, n_jobs=4)\n",
    "# X_train_new, y_train_new = sm.fit_resample(X_train, y_train.ravel())\n",
    "\n",
    "# X_train_new = pd.DataFrame(X_train_new)\n",
    "# X_train_new.columns = X_train.columns\n",
    "# y_train_new = pd.DataFrame(y_train_new)\n",
    "\n",
    "# print('After OverSampling, the shape of X_train_new: {}'.format(X_train_new.shape))\n",
    "# print('After OverSampling, the shape of y_train_new: {} \\n'.format(y_train_new.shape))\n",
    "\n",
    "# print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
    "# print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb93811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "# import gc\n",
    "\n",
    "# EPOCHS = 4\n",
    "# kf = StratifiedKFold(n_splits=EPOCHS, random_state=99, shuffle=True)\n",
    "\n",
    "# y_preds = np.zeros(X_test.shape[0])\n",
    "# y_oof = np.zeros(X_train_new.shape[0])\n",
    "\n",
    "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_new, y_train_new)):\n",
    "#     print(f\"\\nüöÄ Fold {fold + 1}/{EPOCHS}\")\n",
    "    \n",
    "#     X_tr, X_val = X_train_new.iloc[tr_idx], X_train_new.iloc[val_idx]\n",
    "#     y_tr, y_val = y_train_new.iloc[tr_idx], y_train_new.iloc[val_idx]\n",
    "\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         n_estimators=500,\n",
    "#         max_depth=17,\n",
    "#         learning_rate=0.03,\n",
    "#         subsample=0.9,\n",
    "#         colsample_bytree=0.9,\n",
    "#         tree_method='hist',\n",
    "#         use_label_encoder=False,\n",
    "#         eval_metric='auc',\n",
    "#         missing=-999,\n",
    "#         random_state=42\n",
    "#     )\n",
    "\n",
    "#     model.fit(X_tr, y_tr)\n",
    "#     val_preds = model.predict_proba(X_val)[:, 1]\n",
    "#     test_preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     y_oof[val_idx] = val_preds\n",
    "#     y_preds += test_preds / EPOCHS\n",
    "\n",
    "#     evaluate_model(\n",
    "#         model,\n",
    "#         X_val,\n",
    "#         y_val,\n",
    "#         X_test,\n",
    "#         y_test,\n",
    "#         threshold=0.5,\n",
    "#         model_name=\"XGBoost\",\n",
    "#         is_lightgbm=False\n",
    "#     )\n",
    "\n",
    "# print(\"\\nüìä Final OOF Evaluation:\")\n",
    "# evaluate_model(\n",
    "#     model,\n",
    "#     X_val,\n",
    "#     y_val,\n",
    "#     X_test,\n",
    "#     y_test,\n",
    "#     threshold=0.5,\n",
    "#     model_name=\"XGBoost\",\n",
    "#     is_lightgbm=False\n",
    "# )\n",
    "# del X_train_new\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d3eb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, X_test = run_feature_engineering(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:05:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:07:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ProductCD: object, card4: object, card6: object, P_emaildomain: object, R_emaildomain: object, M1: object, M2: object, M3: object, M4: object, M5: object, M6: object, M7: object, M8: object, M9: object, id_12: object, id_15: object, id_16: object, id_23: object, id_27: object, id_28: object, id_29: object, id_30: object, id_31: object, id_33: object, id_34: object, id_35: object, id_36: object, id_37: object, id_38: object, DeviceType: object, DeviceInfo: object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:407\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     new_feature_types.append(\u001b[43m_pandas_dtype_mapper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'object'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      1\u001b[39m model = xgb.XGBClassifier(\n\u001b[32m      2\u001b[39m     n_estimators=\u001b[32m500\u001b[39m,\n\u001b[32m      3\u001b[39m     max_depth=\u001b[32m17\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     random_state=\u001b[32m42\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m model.fit(X_train, y_train)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m f1 = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ F1-score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\code\\University\\Machine-Learning-Project\\nbs\\preprocessing.py:122\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(X_train, X_val, model, y_train, y_val)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(X_train: pd.DataFrame, X_val: pd.DataFrame, model, y_train: pd.Series, y_val: pd.Series) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    121\u001b[39m     model.fit(X_train, y_train)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     y_pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f1_score(y_val, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py:1718\u001b[39m, in \u001b[36mXGBClassifier.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1707\u001b[39m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m   1709\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1715\u001b[39m     iteration_range: Optional[IterationRange] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1716\u001b[39m ) -> ArrayLike:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity=\u001b[38;5;28mself\u001b[39m.verbosity):\n\u001b[32m-> \u001b[39m\u001b[32m1718\u001b[39m         class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1719\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1725\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[32m   1726\u001b[39m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[32m   1727\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:2665\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2663\u001b[39m     data = pd.DataFrame(data)\n\u001b[32m   2664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m2665\u001b[39m     data, fns, _ = \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[32m   2667\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_features(fns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:640\u001b[39m, in \u001b[36m_transform_pandas_df\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data.columns) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot have multiple columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m feature_names, feature_types = \u001b[43mpandas_feature_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:409\u001b[39m, in \u001b[36mpandas_feature_info\u001b[39m\u001b[34m(data, meta, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m    407\u001b[39m             new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m             \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    412\u001b[39m     feature_types = new_feature_types\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\data.py:372\u001b[39m, in \u001b[36m_invalid_dataframe_dtype\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    370\u001b[39m type_err = \u001b[33m\"\u001b[39m\u001b[33mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:ProductCD: object, card4: object, card6: object, P_emaildomain: object, R_emaildomain: object, M1: object, M2: object, M3: object, M4: object, M5: object, M6: object, M7: object, M8: object, M9: object, id_12: object, id_15: object, id_16: object, id_23: object, id_27: object, id_28: object, id_29: object, id_30: object, id_31: object, id_33: object, id_34: object, id_35: object, id_36: object, id_37: object, id_38: object, DeviceType: object, DeviceInfo: object"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=17,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    tree_method='hist',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    missing=-999,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba984414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id_26', 'V305', 'V108', 'V113', 'V111', 'V118', 'V122', 'id_23', 'V123', 'V300', 'id_08', 'V115', 'V110', 'id_22', 'V107', 'V116', 'V311', 'V286', 'id_21', 'C3', 'V109', 'V117', 'id_27', 'V112', 'V120', 'id_07', 'V119', 'V121', 'id_25', 'id_24', 'V301', 'V114']\n",
      "(94487, 400)\n",
      "(94487, 400)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify high missing columns and big top value columns in X_val\n",
    "high_missing_cols_X_val = [col for col in X_val.columns if X_val[col].isnull().sum() / X_val.shape[0] > 0.96]\n",
    "big_top_value_cols_X_val = [col for col in X_val.columns if X_val[col].value_counts(dropna=False, normalize=True).values[0] > 0.96]\n",
    "\n",
    "# Step 2: Combine columns to drop for X_val (same logic as X_train and X_test)\n",
    "cols_to_drop_X_val = list(set(high_missing_cols_X_val + big_top_value_cols_X_val))\n",
    "print(cols_to_drop_X_val)\n",
    "\n",
    "# Step 3: Drop identified columns from X_val\n",
    "X_val = X_val.drop(cols_to_drop_X_val, axis=1)\n",
    "\n",
    "# Step 4: Drop 'TransactionDT' column from X_val\n",
    "X_val.drop('TransactionDT', axis=1, inplace=True)\n",
    "\n",
    "# Step 5: Print the shape of X_val after changes\n",
    "print(X_val.shape)\n",
    "\n",
    "# Step 6: Label Encoding for categorical columns in X_val (same as X_train and X_test)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "for f in X_val.columns:\n",
    "    if X_val[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_val[f].values))  # Fit only on X_val\n",
    "        X_val[f] = lbl.transform(list(X_val[f].values))\n",
    "\n",
    "# Step 7: Fill missing values in X_val (same as X_train and X_test)\n",
    "X_val = X_val.fillna(-999)\n",
    "\n",
    "# Print the shape of X_val after all transformations\n",
    "print(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641e3796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7372034024772423\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810c7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöß Starting feature engineering pipeline...\n",
      "\n",
      "‚úÖ Low-information columns dropped\n",
      "‚úÖ TransactionDT dropped\n",
      "‚úÖ Data cleaned\n",
      "‚úÖ Categorical columns encoded\n",
      "‚úÖ Missing values filled\n",
      "‚úÖ Unused columns dropped\n",
      "üéØ Final shape: (377945, 397)\n",
      "üöß Starting feature engineering pipeline...\n",
      "\n",
      "‚úÖ Low-information columns dropped\n",
      "‚úÖ TransactionDT dropped\n",
      "‚úÖ Data cleaned\n",
      "‚úÖ Categorical columns encoded\n",
      "‚úÖ Missing values filled\n",
      "‚úÖ Unused columns dropped\n",
      "üéØ Final shape: (94487, 397)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ahmed Osama\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:00:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1-score: 0.6727236816116927\n"
     ]
    }
   ],
   "source": [
    "# config = {\n",
    "#     'drop_low_information_columns': True,\n",
    "#     'drop_transaction_dt': True,\n",
    "#     'encode_categorical_columns': True,\n",
    "#     'fill_missing_values': True,\n",
    "#     'create_transaction_amount_ratios': False,\n",
    "#     'group_rare_categories': False,\n",
    "#     'create_time_features': False,\n",
    "#     'drop_unused_columns': True,\n",
    "#     'log_transform_transaction_amt': False\n",
    "# }\n",
    "\n",
    "\n",
    "# X_train_processed = run_feature_engineering_single_df(X_train.copy(), config)\n",
    "# X_val_processed = run_feature_engineering_single_df(X_val.copy(), config)\n",
    "\n",
    "# f1 = evaluate_model(X_train_processed, X_val_processed, model, y_train, y_val)\n",
    "# print(f\"‚úÖ F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5801a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating config: {'create_transaction_amount_ratios': False, 'clean_data': True, 'group_rare_categories': False, 'encode_categorical_columns': True, 'fill_missing_values': True, 'create_time_features': False, 'drop_unused_columns': False, 'log_transform_transaction_amt': True}\n",
      "üöß Starting feature engineering pipeline...\n",
      "\n",
      "‚úÖ Low-information columns dropped\n",
      "‚úÖ TransactionDT dropped\n",
      "‚úÖ Data cleaned\n",
      "‚úÖ Categorical columns encoded\n",
      "‚úÖ Missing values filled\n",
      "‚úÖ Log transformation applied to TransactionAmt\n",
      "üéØ Final shape: (377945, 401)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\code\\University\\Machine-Learning-Project\\nbs\\preprocessing.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X['TransactionAmt_log'] = np.log1p(X['TransactionAmt'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöß Starting feature engineering pipeline...\n",
      "\n",
      "‚úÖ Low-information columns dropped\n",
      "‚úÖ TransactionDT dropped\n"
     ]
    }
   ],
   "source": [
    "best_config = search_best_config(X_train, X_val, model, y_train, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
